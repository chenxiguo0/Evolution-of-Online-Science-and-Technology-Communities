[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evolution of Online Science and Technology Communities on Reddit",
    "section": "",
    "text": "Chenxi Guo\nLinjin He\nXiaoya Meng"
  },
  {
    "objectID": "index.html#team-members",
    "href": "index.html#team-members",
    "title": "Evolution of Online Science and Technology Communities on Reddit",
    "section": "",
    "text": "Chenxi Guo\nLinjin He\nXiaoya Meng"
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Evolution of Online Science and Technology Communities on Reddit",
    "section": "Project Overview",
    "text": "Project Overview\nHigh-Level Problem Statement:\nHow do online science and technology communities on Reddit evolve, interact, and structure their discussions around AI and emerging technologies?\n\nDataset\n\nSource: Reddit Comments and Submissions\nTime Period: 2023-06-01 to 2024-07-31\nSubreddits: A curated list of science, technology, and AI-related subreddits, including ChatGPT, technology, ArtificialInteligence, MachineLearning, Futurology, datascience, OpenAI, and more.\nScale: ~13.4 million rows (~12.6M comments, ~0.8M submissions)\nSize: ~13.4 GB\n\n\n\nBusiness Questions\nWe address 10 business questions spanning three analytical approaches:\nExploratory Data Analysis (EDA):\n\nHow has community activity evolved across science, technology, and AI subreddits over time?\nWhich technology-related subreddits demonstrate the strongest user engagement and retention over time?\nHow concentrated is attention within technology-related discussions (comments and scores)?\nDo science, technology, and AI subreddits share overlapping user communities?\n\nNatural Language Processing (NLP):\n\nWhat are the dominant topics and trends within fast-growing technology-related subreddits?\nWhat are the baseline emotional patterns of discussions about AI and emerging technologies?\nHow do external technological or policy events disrupt or reshape existing discussion patterns in online technology-related communities?\nHow do users shape topic emphasis and sentiment dynamics across science, technology, and AI subreddits?\n\nMachine Learning (ML):\n\nCan high-quality Reddit comments in science, technology, and AI subreddits be predicted from comment content and basic behavioral features?\nCan distinct discussion communities be identified within technology-related subreddits based on patterns of language use?\n\nSee BUSINESS_QUESTIONS.md for detailed technical approaches."
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Evolution of Online Science and Technology Communities on Reddit",
    "section": "Methodology",
    "text": "Methodology\n\nData Processing Infrastructure\n\nPlatform: Apache Spark cluster on AWS EC2\nProcessing: Distributed computing with PySpark\nStorage: Amazon S3 for data lake architecture\nScale: Processing over 13 million rows of text and metadata\n\n\n\nAnalysis Pipeline\n\nData Acquisition & Filtering\n\nCopied and filtered a large public Reddit dataset to relevant subreddits and timeframes.\n\nExploratory Data Analysis → EDA Page\n\nAnalyzed temporal patterns, user engagement, attention concentration, and community overlap.\n\nNatural Language Processing → NLP Page\n\nDeployed topic modeling, sentiment analysis, and rule-based text classification.\n\nMachine Learning → ML Page\n\nTrained and evaluated classification and clustering models to predict content quality and identify user segments.\n\nFinal Analysis → Conclusion\n\nSynthesized all findings into actionable insights and business recommendations."
  },
  {
    "objectID": "index.html#key-findings-preview",
    "href": "index.html#key-findings-preview",
    "title": "Evolution of Online Science and Technology Communities on Reddit",
    "section": "Key Findings Preview",
    "text": "Key Findings Preview\n\nMajor Insights\n\nCommunity Engagement is Event-Driven but Emotionally Stable: User activity surges in response to major AI product launches and news, but the overall emotional tone of the communities remains consistently neutral-to-positive, showing high resilience.\nA Core, Interconnected AI Community Drives the Conversation: A handful of central AI-focused subreddits (like r/ChatGPT, r/OpenAI) share a large, active user base, acting as the primary hub for discourse that spreads to the wider tech ecosystem.\nPredictive Models Can Augment, Not Replace, Human Moderation: Our machine learning model can successfully identify the majority of high-quality comments (high recall), but its lower precision makes it best suited as a tool to assist human moderators, not fully automate their work.\n\n\n\nBusiness Impact\nOur findings provide a roadmap for understanding and engaging with online technology communities. By identifying key hubs of conversation, understanding the stable emotional climate, and leveraging ML for content curation, organizations can develop more efficient and impactful communication and community management strategies."
  },
  {
    "objectID": "index.html#navigation",
    "href": "index.html#navigation",
    "title": "Evolution of Online Science and Technology Communities on Reddit",
    "section": "Navigation",
    "text": "Navigation\nUse the navigation bar above to explore:\n\nEDA: Exploratory data analysis with statistical insights and temporal patterns\nNLP: Natural language processing analysis including sentiment and topics\nML: Machine learning models and predictions\nConclusion: Summary of findings and recommendations\nReport: Comprehensive analysis and results"
  },
  {
    "objectID": "index.html#repository",
    "href": "index.html#repository",
    "title": "Evolution of Online Science and Technology Communities on Reddit",
    "section": "Repository",
    "text": "Repository\n\nGitHub: https://github.com/gu-dsan6000/fall-2025-project-team29\nDocumentation: Comprehensive project details, methodology, and results are presented on this website. All source code and data outputs can be found in the GitHub repository.\n\n\nLast updated: December 6, 2025"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion and Recommendations",
    "section": "",
    "text": "This project analyzed the evolution of science and technology communities on Reddit to understand how they shape public sentiment and discourse around AI and emerging technologies. By processing over 13 million Reddit submissions and comments from June 2023 to July 2024 using Apache Spark, we conducted a comprehensive analysis spanning Exploratory Data Analysis (EDA), Natural Language Processing (NLP), and Machine Learning (ML). Our findings reveal that community activity is highly event-driven, discussions are predominantly neutral-to-positive but can be influenced by major tech announcements, and distinct user communities exist with varying levels of engagement and topic concentration. The machine learning models we developed provide a framework for identifying high-quality content and segmenting discussions, offering valuable tools for community managers, marketers, and researchers aiming to understand and engage with these dynamic online ecosystems."
  },
  {
    "objectID": "conclusion.html#executive-summary",
    "href": "conclusion.html#executive-summary",
    "title": "Conclusion and Recommendations",
    "section": "",
    "text": "This project analyzed the evolution of science and technology communities on Reddit to understand how they shape public sentiment and discourse around AI and emerging technologies. By processing over 13 million Reddit submissions and comments from June 2023 to July 2024 using Apache Spark, we conducted a comprehensive analysis spanning Exploratory Data Analysis (EDA), Natural Language Processing (NLP), and Machine Learning (ML). Our findings reveal that community activity is highly event-driven, discussions are predominantly neutral-to-positive but can be influenced by major tech announcements, and distinct user communities exist with varying levels of engagement and topic concentration. The machine learning models we developed provide a framework for identifying high-quality content and segmenting discussions, offering valuable tools for community managers, marketers, and researchers aiming to understand and engage with these dynamic online ecosystems."
  },
  {
    "objectID": "conclusion.html#answers-to-all-10-business-questions",
    "href": "conclusion.html#answers-to-all-10-business-questions",
    "title": "Conclusion and Recommendations",
    "section": "Answers to All 10 Business Questions",
    "text": "Answers to All 10 Business Questions\n\nEDA Questions\n\nHow has community activity evolved across science, technology, and AI subreddits over time?: Community activity, measured by posts and comments, shows significant temporal fluctuations. Subreddits like r/ChatGPT, r/technology, and r/Futurology exhibit notable spikes in engagement that correlate with major industry events, such as new AI model launches in early and mid-2024.\nWhich technology-related subreddits demonstrate the strongest user engagement and retention over time?: Established communities like r/MachineLearning and r/ChatGPT maintain higher and more stable user retention ratios, indicating strong community “stickiness.” In contrast, smaller or more niche subreddits display more volatile engagement, with a higher proportion of one-time contributors.\nHow concentrated is attention within technology-related discussions?: Attention is highly concentrated in large, general-interest subreddits like r/technology (Gini coefficient &gt; 0.9), where a few viral posts dominate discussions. Specialized communities such as r/datascience show more egalitarian patterns, with engagement spread more evenly across posts.\nDo science, technology, and AI subreddits share overlapping user communities?: Yes, there is strong user overlap among core AI-focused subreddits (r/ChatGPT, r/OpenAI, r/ArtificialInteligence), indicating a shared, highly engaged user base. General tech forums like r/technology act as bridges, connecting the AI-centric groups with a broader audience.\n\n\n\nNLP Questions\n\nWhat are the dominant topics and trends within fast-growing technology-related subreddits?: Using LDA topic modeling, we identified ten dominant topics. The most prevalent were informal conversations (22.4%), discussions on human-AI relations and future technologies (13.6%), and career/learning-focused content (13%). These topic distributions remained remarkably stable over the one-year period.\nWhat are the baseline emotional patterns of discussions about AI and emerging technologies?: The overwhelming majority of discussions are emotionally neutral. However, there is a consistent positive skew, with more positive comments than negative ones across nearly all subreddits. The average sentiment remains in the neutral-to-mildly-positive range, suggesting a generally constructive atmosphere.\nHow do external technological or policy events disrupt or reshape existing discussion patterns?: While overall sentiment is stable, the AI/ML-focused subreddits show noticeable, albeit temporary, increases in positive sentiment following major events like the Gemini launch and the passing of the EU AI Act. Topic trends, however, remain largely undisrupted, indicating that the core discussion themes are resilient to short-term events.\nHow do users shape topic emphasis and sentiment dynamics across science, technology, and AI subreddits?: Through rule-based classification, we identified distinct vocabularies for different discussion types. “Ethical” discussions feature words like regulation and bias, “societal” discussions focus on jobs and impact, and “technical” discussions are dominated by terms like model and data. This shows that users adopt specific language to frame their conversations, shaping the thematic focus of each category.\n\n\n\nML Questions\n\nCan high-quality Reddit comments be predicted?: Yes, but with trade-offs. Our Logistic Regression model, trained to predict comments with a score of 6 or higher, achieved a high recall (70.6%) but low precision (22.3%). This means it is effective at identifying a majority of high-quality comments but also produces many false positives, making it suitable for flagging content for human review rather than for fully automated moderation.\nCan distinct discussion communities be identified based on language use?: Yes. K-Means clustering on text features successfully segmented discussions into meaningful thematic groups. We identified a large, general-discussion cluster, a conversational cluster, and a smaller, highly coherent technical cluster focused on programming and data science. These segments can be used for more targeted analysis or feature engineering."
  },
  {
    "objectID": "conclusion.html#major-findings",
    "href": "conclusion.html#major-findings",
    "title": "Conclusion and Recommendations",
    "section": "Major Findings",
    "text": "Major Findings\n\nKey Insight 1: Community Engagement is Event-Driven but Emotionally Stable\nOur EDA and NLP analyses revealed that while user activity (posts, comments) surges in response to external events like AI product launches, the overall sentiment of the communities remains remarkably stable and neutral-to-positive. Business Impact: This suggests that stakeholders should not overreact to short-term activity spikes. While these events draw attention, they do not fundamentally alter the underlying positive and constructive tone of the communities. Marketing and engagement strategies can be planned around major tech milestones to maximize reach, without needing to constantly adapt to perceived emotional volatility.\n\n\nKey Insight 2: A Core, Interconnected AI Community Drives the Conversation\nUser overlap analysis (Jaccard similarity) demonstrated that a handful of core AI subreddits (r/ChatGPT, r/OpenAI, r/ArtificialInteligence) share a significant and active user base. These communities function as the central hub of the AI discourse on Reddit. Business Impact: For organizations looking to influence or understand the AI conversation, focusing engagement efforts on this core set of interconnected communities is far more efficient than a scattered approach. These are the places where ideas are born and disseminated to the wider tech ecosystem.\n\n\nKey Insight 3: Predictive Models Can Augment, Not Replace, Human Moderation\nOur ML classification model for predicting high-quality comments showed a classic precision-recall trade-off. While it could find most of the “good” comments, it also flagged many “bad” ones. Business Impact: This highlights a critical lesson for applying AI in community management. The model is not a turnkey solution for automated moderation but a powerful tool for augmenting human efforts. It can be used to build a “priority queue” for moderators or to identify promising content for promotion, thereby improving efficiency."
  },
  {
    "objectID": "conclusion.html#addressing-the-high-level-problem",
    "href": "conclusion.html#addressing-the-high-level-problem",
    "title": "Conclusion and Recommendations",
    "section": "Addressing the High-Level Problem",
    "text": "Addressing the High-Level Problem\nOriginal Problem Statement: How do online science and technology communities on Reddit evolve and shape public understanding and sentiment toward AI and emerging technologies?\nHow Our Analysis Addresses It: Our comprehensive analysis provides a multi-faceted answer to this question. We demonstrated that these communities evolve in response to real-world events, with activity levels serving as a barometer for public interest (BQ1, BQ7). We quantified their structure, revealing that engagement is not uniform; some communities are “sticky” and retain users (BQ2), while others have highly concentrated “viral” discussions (BQ3). A core group of interconnected subreddits acts as the central nervous system for AI discourse (BQ4).\nFurthermore, we mapped the substance of these conversations. We found that the discourse is thematically diverse, covering technical, ethical, and societal dimensions (BQ5, BQ8), and maintains a surprisingly stable, neutral-to-positive emotional baseline (BQ6). Finally, our ML models provide a forward-looking capability, offering methods to identify high-quality content (BQ9) and segment the audience based on language (BQ10), which are crucial steps in actively shaping public understanding. Together, these findings paint a detailed picture of not just how these communities evolve, but what drives their evolution and shapes their internal dynamics."
  },
  {
    "objectID": "conclusion.html#business-recommendations",
    "href": "conclusion.html#business-recommendations",
    "title": "Conclusion and Recommendations",
    "section": "Business Recommendations",
    "text": "Business Recommendations\n\nRecommendation 1: Target Core AI Subreddits for High-Impact Engagement\nBased on: EDA findings on user overlap (BQ4) and attention concentration (BQ3). Action: Focus community management, marketing, and research efforts on the highly interconnected cluster of r/ChatGPT, r/OpenAI, and r/ArtificialInteligence. Use the “viral” nature of these forums to launch announcements or seed discussions. Expected Impact: More efficient use of resources, leading to higher visibility and greater influence on the central AI conversation.\n\n\nRecommendation 2: Use Sentiment Analysis as a Barometer, Not an Alarm\nBased on: NLP findings on sentiment stability and event-driven fluctuations (BQ6, BQ7). Action: Implement a monitoring dashboard that tracks sentiment in key subreddits. Use it to gauge the reception of new products or news, but avoid making drastic strategy changes based on short-term emotional shifts. The baseline is positive; focus on long-term trends. Expected Impact: More resilient and data-informed communication strategies that are proactive rather than reactive, fostering trust within the community.\n\n\nRecommendation 3: Implement an ML-Augmented Content Curation System\nBased on: ML findings from the quality prediction model (BQ9). Action: Deploy the logistic regression model to flag promising user-generated comments and posts. Feed these flagged items to a human-in-the-loop system where community managers can quickly identify and promote high-quality content. Expected Impact: Increased visibility for constructive discussions, improved community health, and a more efficient workflow for content curation and moderation."
  },
  {
    "objectID": "conclusion.html#limitations-and-future-work",
    "href": "conclusion.html#limitations-and-future-work",
    "title": "Conclusion and Recommendations",
    "section": "Limitations and Future Work",
    "text": "Limitations and Future Work\n\nLimitations\n\nSentiment Analysis Nuance: VADER is a lexicon-based tool and may not capture complex sarcasm or context-specific technical jargon. Its accuracy could be limited.\nRepresentativeness of Reddit: While large, our dataset is confined to Reddit and may not represent the full spectrum of public opinion on science and technology.\nStatic Topic Model: Our LDA model identifies topics from the entire corpus, which may not fully capture the evolution of topics over time.\n\n\n\nFuture Work\n\nAdvanced Sentiment Models: Fine-tune a transformer-based model (like BERT) on domain-specific data to achieve more accurate sentiment classification.\nDynamic Topic Modeling: Implement a Dynamic Topic Model (DTM) to explicitly track how the prevalence and meaning of topics change over time and in response to events.\nCausal Inference: Move beyond correlation to causal analysis by using statistical methods to determine the causal impact of specific events (e.g., a product launch) on community growth and sentiment."
  },
  {
    "objectID": "conclusion.html#technical-achievements",
    "href": "conclusion.html#technical-achievements",
    "title": "Conclusion and Recommendations",
    "section": "Technical Achievements",
    "text": "Technical Achievements\n\nBig Data Processing\n\nProcessed ~13.4 million rows of Reddit data (~12.6M comments, 0.8M submissions).\nUtilized a multi-node Apache Spark cluster on AWS for all data aggregation and model training, enabling analysis at a scale impossible on a single machine.\nAchieved efficient processing pipelines for EDA, NLP, and ML tasks.\n\n\n\nAnalytical Breadth\n\nEDA: Conducted large-scale statistical analysis of user activity, retention (user-set intersections), attention concentration (Gini coefficient), and community overlap (Jaccard similarity).\nNLP: Deployed scalable topic modeling (LDA), sentiment analysis (VADER), and rule-based text classification across millions of documents.\nML: Built and evaluated a binary classification (Logistic Regression) and an unsupervised clustering (K-Means) model using Spark MLlib, including feature engineering and performance evaluation on a massive dataset."
  },
  {
    "objectID": "conclusion.html#lessons-learned",
    "href": "conclusion.html#lessons-learned",
    "title": "Conclusion and Recommendations",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nTechnical Lessons\n\nSpark is Essential for Scale: Analyzing millions of documents and user interactions would have been computationally infeasible without Spark’s distributed processing capabilities.\nFeature Engineering is Key: The performance of our ML models was heavily dependent on the quality of features engineered from raw text and metadata. Simple models with good features can be surprisingly effective.\nInfrastructure Automation: Automating the setup and teardown of the Spark cluster was critical for iterative development and cost management.\n\n\n\nDomain Lessons\n\nReddit is Not a Monolith: Tech and science communities on Reddit are incredibly diverse, ranging from highly technical forums to broad, future-focused discussion hubs. A one-size-fits-all approach to analysis or engagement is bound to fail.\nConversation is Resilient: Despite the fast-paced news cycle, the core themes and emotional tone of these communities are remarkably stable, pointing to a mature and established discourse."
  },
  {
    "objectID": "conclusion.html#final-thoughts",
    "href": "conclusion.html#final-thoughts",
    "title": "Conclusion and Recommendations",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis project successfully navigated the complexities of big data analytics to transform a massive, unstructured dataset into a clear narrative about the evolution of online science and technology communities. We moved from foundational data processing and exploration to sophisticated natural language processing and predictive modeling, uncovering actionable insights at each stage. Our findings not only answer our initial business questions but also provide a robust framework for any organization seeking to engage with and understand the powerful forces shaping public discourse on technology."
  },
  {
    "objectID": "conclusion.html#acknowledgments",
    "href": "conclusion.html#acknowledgments",
    "title": "Conclusion and Recommendations",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nTeam Members: Chenxi Guo, Linjin He, Xiaoya Meng\nCourse: DSAN 6000 Big Data Analytics and Cloud Computing\nTools: Apache Spark, PySpark, Quarto, Python\nData Source: Reddit via Pushshift archives"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "ImportantMilestone 2 Deliverable\n\n\n\nThis page is populated during Milestone 2 (Week 5) with NLP findings."
  },
  {
    "objectID": "nlp.html#overview",
    "href": "nlp.html#overview",
    "title": "Natural Language Processing",
    "section": "Overview",
    "text": "Overview\nThis page presents NLP analysis addressing [3-4] NLP business questions through sentiment analysis, topic modeling, and text mining."
  },
  {
    "objectID": "nlp.html#business-question-1-your-nlp-question",
    "href": "nlp.html#business-question-1-your-nlp-question",
    "title": "Natural Language Processing",
    "section": "Business Question 1: [Your NLP Question]",
    "text": "Business Question 1: [Your NLP Question]\nQuestion: [Full question text]\n\nFindings\n[Visualization placeholder]\nKey Insights:\n\n[Insight 1]\n[Insight 2]"
  },
  {
    "objectID": "nlp.html#sentiment-analysis",
    "href": "nlp.html#sentiment-analysis",
    "title": "Natural Language Processing",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nOverall Sentiment Distribution\n[Description and visualization]\n\n\nSentiment by Subreddit\n[Comparison visualization]"
  },
  {
    "objectID": "nlp.html#topic-modeling",
    "href": "nlp.html#topic-modeling",
    "title": "Natural Language Processing",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\nDiscovered Topics\nTopic 1: [Topic Name]\n\nKeywords: [keywords]\n\nTopic 2: [Topic Name]\n\nKeywords: [keywords]"
  },
  {
    "objectID": "nlp.html#summary",
    "href": "nlp.html#summary",
    "title": "Natural Language Processing",
    "section": "Summary",
    "text": "Summary\n\nAnswers to NLP Business Questions\n\n[Question 1]: [Answer]\n[Question 2]: [Answer]\n[Question 3]: [Answer]\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll NLP code is in code/nlp/ directory."
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Milestone 3 Deliverable\n\n\n\nThis page is populated during Milestone 3 (Week 6) with ML findings."
  },
  {
    "objectID": "ml.html#overview",
    "href": "ml.html#overview",
    "title": "Machine Learning",
    "section": "Overview",
    "text": "Overview\nThis page presents machine learning models addressing [2-3] ML business questions through classification, regression, or clustering."
  },
  {
    "objectID": "ml.html#business-question-1-your-ml-question",
    "href": "ml.html#business-question-1-your-ml-question",
    "title": "Machine Learning",
    "section": "Business Question 1: [Your ML Question]",
    "text": "Business Question 1: [Your ML Question]\nQuestion: [Full question text]\n\nProblem Formulation\n\nTask Type: [Classification / Regression / Clustering]\nTarget Variable: [What you’re predicting]\nEvaluation Metric: [Primary metric]\n\n\n\nFeature Engineering\nFeatures Used:\n\n[Feature category 1]\n[Feature category 2]\n[Feature category 3]\n\n\n\nModel Performance\n[Visualization: ROC curve, confusion matrix, etc.]\nResults:\n\nAccuracy: [value]\nPrecision: [value]\nRecall: [value]\nF1 Score: [value]"
  },
  {
    "objectID": "ml.html#feature-importance",
    "href": "ml.html#feature-importance",
    "title": "Machine Learning",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nTop 10 Most Important Features\n[Visualization and table]"
  },
  {
    "objectID": "ml.html#model-comparison",
    "href": "ml.html#model-comparison",
    "title": "Machine Learning",
    "section": "Model Comparison",
    "text": "Model Comparison\n[Table comparing different models tried]"
  },
  {
    "objectID": "ml.html#summary",
    "href": "ml.html#summary",
    "title": "Machine Learning",
    "section": "Summary",
    "text": "Summary\n\nAnswers to ML Business Questions\n\n[Question 1]: [Answer]\n[Question 2]: [Answer]\n\n\n\nBusiness Implications\n[How models can be applied in practice]\n\n\n\n\n\n\n\nTip\n\n\n\nAll ML code is in code/ml/ directory. Models saved in code/ml/models/."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "ImportantMilestone 1 Deliverable\n\n\n\nThis page is populated during Milestone 1 (Week 3) with EDA findings."
  },
  {
    "objectID": "eda.html#overview",
    "href": "eda.html#overview",
    "title": "Exploratory Data Analysis",
    "section": "Overview",
    "text": "Overview\nThis page presents exploratory data analysis of our Reddit dataset, addressing [3-4] EDA business questions through statistical analysis, temporal patterns, and visualizations."
  },
  {
    "objectID": "eda.html#data-overview",
    "href": "eda.html#data-overview",
    "title": "Exploratory Data Analysis",
    "section": "Data Overview",
    "text": "Data Overview\n\nDataset Summary\nKey Statistics:\n\nTotal Comments: [Number]\nTotal Submissions: [Number]\nDate Range: [Start] to [End]\nNumber of Subreddits: [Number]"
  },
  {
    "objectID": "eda.html#business-question-1-your-eda-question",
    "href": "eda.html#business-question-1-your-eda-question",
    "title": "Exploratory Data Analysis",
    "section": "Business Question 1: [Your EDA Question]",
    "text": "Business Question 1: [Your EDA Question]\nQuestion: [Full question text]\n\nAnalysis Approach\n[Brief description of methodology]\n\n\nFindings\n[Placeholder for visualization]\nKey Insights:\n\n[Insight 1]\n[Insight 2]\n[Insight 3]"
  },
  {
    "objectID": "eda.html#business-question-2-your-eda-question",
    "href": "eda.html#business-question-2-your-eda-question",
    "title": "Exploratory Data Analysis",
    "section": "Business Question 2: [Your EDA Question]",
    "text": "Business Question 2: [Your EDA Question]\nQuestion: [Full question text]\n\nAnalysis Approach\n[Brief description of methodology]\n\n\nFindings\n[Placeholder for visualization]\nKey Insights:\n\n[Insight 1]\n[Insight 2]\n[Insight 3]"
  },
  {
    "objectID": "eda.html#temporal-analysis",
    "href": "eda.html#temporal-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Temporal Analysis",
    "text": "Temporal Analysis\n\nPosting Patterns Over Time\n[Description of temporal patterns discovered]\n\n\nPeak Activity Analysis\nHourly Patterns:\n\nPeak hours: [Hours]\nLowest activity: [Hours]\n\nWeekly Patterns:\n\nMost active days: [Days]\nLeast active days: [Days]"
  },
  {
    "objectID": "eda.html#summary",
    "href": "eda.html#summary",
    "title": "Exploratory Data Analysis",
    "section": "Summary",
    "text": "Summary\n\nAnswers to EDA Business Questions\n\n[Question 1]: [Concise answer]\n[Question 2]: [Concise answer]\n[Question 3]: [Concise answer]\n\n\n\nKey Takeaways\n\n[Major finding 1]\n[Major finding 2]\n[Major finding 3]\n\n\n\n\n\n\n\n\nTipCode and Data\n\n\n\nAll EDA code is available in code/eda/ directory. Results are saved in data/csv/ and visualizations in data/plots/."
  },
  {
    "objectID": "ML_WEB.html#overview",
    "href": "ML_WEB.html#overview",
    "title": "Machine Learning",
    "section": "Overview",
    "text": "Overview\nIn this milestone, we apply various machine learning techniques using Apache Spark’s MLlib. Our focus is on solving two main business questions through classification and regression, leveraging features engineered from EDA and NLP insights."
  },
  {
    "objectID": "ML_WEB.html#research-question-9-can-we-predict-whether-an-ai-related-post-will-become-highly-engaging",
    "href": "ML_WEB.html#research-question-9-can-we-predict-whether-an-ai-related-post-will-become-highly-engaging",
    "title": "Machine Learning",
    "section": "Research Question 9: Can we predict whether an AI-related post will become highly engaging?",
    "text": "Research Question 9: Can we predict whether an AI-related post will become highly engaging?\nBusiness Question:\nCan we build a model to accurately predict which AI-related posts will achieve high engagement on Reddit?\nAnalysis Approach:\nThis is formulated as a binary classification task. We trained a Logistic Regression model using a rich set of features including post metadata (e.g., subreddit, submission time), and NLP-derived features (e.g., sentiment of title, dominant topic). Model performance is assessed using standard classification metrics.\nFeatures & training details: - Positive label: score &gt;= 6 (see code/ml/ml_Q1.py). - Features: TF-IDF text features (HashingTF + IDF), comment_length, has_url, hour_of_day, day_of_week. - Train/validation/test splits performed in code (approx. 80/20 overall; train/val internal split applied). - Class weighting applied to address imbalance (weightCol used in LogisticRegression). - Model saved at: code/ml/models/logistic_regression.\n\nConfusion Matrix\n\ntrue_0 → pred_0: 949,839 ; pred_1: 1,112,372\ntrue_1 → pred_0: 132,447 ; pred_1: 318,706\n\n\n\nModel performance\n\nAccuracy: 0.5047\nPrecision: 0.2227\nRecall: 0.7064\nF1-score: 0.3386\nAUC: 0.6257\n\n\nconfusion matrix: - Counts: true_0 → pred_0: 949,839 ; pred_1: 1,112,372; true_1 → pred_0: 132,447 ; pred_1: 318,706. - This indicates the model predicts many samples as positive (pred_1), producing a substantial number of false positives. The confusion matrix helps identify where the model misclassifies and guides threshold tuning or feature improvements.\n\nROC: - The ROC curve shows the model’s trade-off between true positive rate and false positive rate across thresholds. The reported AUC = 0.626 (see metrics file) indicates modest discriminative ability — better than random but room for improvement. - Use this plot to compare models or to choose operating points that balance sensitivity and specificity.\n\nPrecision–Recall: - Precision = 0.223 and Recall = 0.706 (metrics file). The PR curve highlights that although recall is high (the model catches most high-engagement posts), precision is low — many predicted positives are false. - For deployment, consider threshold tuning on the PR curve to meet business needs (e.g., higher precision for automated actions, or higher recall for candidate selection).\nInterpretation & implications: - The model shows high recall but low precision: it successfully identifies most high-engagement posts but produces many false positives. This means the model is useful for candidate selection (e.g., flagging posts for manual review) but not ideal for automated posting strategies without threshold tuning. - AUC ≈ 0.626 indicates modest discriminative power; further work needed to improve robustness.\nFuture improvements: - Use embeddings (BERT / Sentence-BERT) to capture semantic meaning rather than HashingTF alone. - Incorporate richer author and subreddit metadata (author history, subreddit-specific features, prior post performance). - Try stronger classifiers (Gradient Boosting / XGBoost / LightGBM) and tune thresholds by business objective (precision vs recall tradeoffs)."
  },
  {
    "objectID": "ML_WEB.html#research-question-10-can-we-forecast-the-next-months-public-sentiment-toward-ai",
    "href": "ML_WEB.html#research-question-10-can-we-forecast-the-next-months-public-sentiment-toward-ai",
    "title": "Machine Learning",
    "section": "Research Question 10: Can we forecast the next month’s public sentiment toward AI?",
    "text": "Research Question 10: Can we forecast the next month’s public sentiment toward AI?\nBusiness Question:\nIs it possible to predict future sentiment trends regarding AI in Reddit communities, thereby enabling proactive understanding of public mood?\nAnalysis Approach:\nWe explored clustering of subreddit submissions (K-Means) to identify groups with similar sentiment/text signatures as a first step; a supervised regression pipeline for forecasting monthly sentiment remains to be implemented.\n\nCluster analysis & visualization\n\nCluster artifacts and model saved in code/ml/models/kmeans_k13.\nCluster analysis summary: data/csv/ML2_cluster_analysis.csv.\nElbow chart for K selection: data/plots/ML2_elbow_method.png.\n\n\nclustering: - The clustering plot groups subreddits by similar text/sentiment patterns. Key clusters include cluster 10 (largest, many tokens like ‘removed’, ‘ai’, ‘chatgpt’) and cluster 8 (conversational language). - These clusters can be used to build cluster-specific forecasting models or to include cluster id as a feature in regression/classification tasks.\nSelected cluster summary: - Largest cluster id 10: size 43,558 (tokens: ‘removed’, ‘im’, ‘ai’, ‘chatgpt’, ‘gpt’, ‘new’, ‘use’, ‘get’) - Cluster 8: size 4,989 (conversational tokens: ‘im’, ‘like’, ‘time’) - Cluster 6: size 511 (technical tokens: ‘ai’, ‘code’, ‘import’, ‘data’)\nInterpretation & current status: - Clustering reveals meaningful groupings that can improve targeted forecasting (modeling per cluster or using cluster as a feature). - No regression model for next-month sentiment forecasting is saved in the repository; implementing this is recommended as the next step.\nNext steps to implement forecasting: 1. Aggregate monthly features per subreddit/cluster: lagged sentiment, topic shares, activity metrics. 2. Train and cross-validate regression models (GBRT, ElasticNet), evaluate with RMSE/MAE/R2. 3. Use interpretability tools (SHAP) to identify key drivers. 4. Deploy forecasting pipeline and produce monthly forecasts with uncertainty bounds."
  },
  {
    "objectID": "ML_WEB.html#summary",
    "href": "ML_WEB.html#summary",
    "title": "Machine Learning",
    "section": "Summary",
    "text": "Summary\n\nThe Logistic Regression classifier is effective at capturing high-engagement posts (high recall) but has low precision and modest overall discriminative power (AUC ~0.63).\nClustering reveals distinct subreddit groups useful for tailored forecasting and analysis.\nImprovements in text representation, richer features, and model complexity are needed to move from exploratory ML to production-ready predictions."
  },
  {
    "objectID": "ML_WEB.html#business-question-9-can-high-quality-reddit-comments-in-science-technology-and-ai-subreddits-be-predicted-from-comment-content-and-basic-behavioral-features",
    "href": "ML_WEB.html#business-question-9-can-high-quality-reddit-comments-in-science-technology-and-ai-subreddits-be-predicted-from-comment-content-and-basic-behavioral-features",
    "title": "Machine Learning",
    "section": "Business Question 9: Can high-quality Reddit comments in science, technology, and AI subreddits be predicted from comment content and basic behavioral features?",
    "text": "Business Question 9: Can high-quality Reddit comments in science, technology, and AI subreddits be predicted from comment content and basic behavioral features?\nAnalysis Approach:\nThis is formulated as a binary classification task where score &gt;= 6 defines a “high-quality” comment. We trained a Logistic Regression model using text features (TF-IDF) and behavioral features (comment length, time of day, etc.). Model performance is assessed using standard classification metrics.\nFeatures & training details: - Positive label: score &gt;= 6 (see code/ml/ml_Q1.py). - Features: TF-IDF text features (HashingTF + IDF), comment_length, has_url, hour_of_day, day_of_week. - Train/validation/test splits performed in code (approx. 80/20 overall; train/val internal split applied). - Class weighting applied to address imbalance (weightCol used in LogisticRegression). - Model saved at: code/ml/models/logistic_regression.\n\nConfusion Matrix\n\ntrue_0 → pred_0: 949,839 ; pred_1: 1,112,372\ntrue_1 → pred_0: 132,447 ; pred_1: 318,706\n\n\n\nModel performance\n\nAccuracy: 0.5047\nPrecision: 0.2227\nRecall: 0.7064\nF1-score: 0.3386\nAUC: 0.6257\n\n\nconfusion matrix: - Counts: true_0 → pred_0: 949,839 ; pred_1: 1,112,372; true_1 → pred_0: 132,447 ; pred_1: 318,706. - This indicates the model predicts many samples as positive (pred_1), producing a substantial number of false positives. The confusion matrix helps identify where the model misclassifies and guides threshold tuning or feature improvements.\n\nROC: - The ROC curve shows the model’s trade-off between true positive rate and false positive rate across thresholds. The reported AUC = 0.626 (see metrics file) indicates modest discriminative ability — better than random but room for improvement. - Use this plot to compare models or to choose operating points that balance sensitivity and specificity.\n\nPrecision–Recall: - Precision = 0.223 and Recall = 0.706 (metrics file). The PR curve highlights that although recall is high (the model catches most high-engagement posts), precision is low — many predicted positives are false. - For deployment, consider threshold tuning on the PR curve to meet business needs (e.g., higher precision for automated actions, or higher recall for candidate selection).\nInterpretation & implications: - The model shows high recall (70.6%) but low precision (22.3%). - Specifically: It correctly identified 318,706 high-quality comments (True Positives) but misclassified 1,112,372 low-quality ones as high-quality (False Positives). - This means the model is effective at finding a majority of the high-quality content but also produces a large number of false alarms. It is best suited for tasks like flagging content for human review, rather than for fully automated decision-making where precision is paramount. - The overall AUC of 0.626 indicates modest discriminative power, suggesting that while the model is better than random chance, there is significant room for improvement.\nFuture improvements: - Use embeddings (BERT / Sentence-BERT) to capture semantic meaning rather than HashingTF alone. - Incorporate richer author and subreddit metadata (author history, subreddit-specific features, prior post performance). - Try stronger classifiers (Gradient Boosting / XGBoost / LightGBM) and tune thresholds by business objective (precision vs recall tradeoffs)."
  },
  {
    "objectID": "ML_WEB.html#business-question-10-can-distinct-discussion-communities-be-identified-within-technology-related-subreddits-based-on-patterns-of-language-use",
    "href": "ML_WEB.html#business-question-10-can-distinct-discussion-communities-be-identified-within-technology-related-subreddits-based-on-patterns-of-language-use",
    "title": "Machine Learning",
    "section": "Business Question 10: Can distinct discussion communities be identified within technology-related subreddits based on patterns of language use?",
    "text": "Business Question 10: Can distinct discussion communities be identified within technology-related subreddits based on patterns of language use?\nAnalysis Approach:\nWe applied K-Means clustering to the TF-IDF features of subreddit submissions. The goal is to identify groups of subreddits that share similar textual content and language use. The elbow method was used to determine the optimal number of clusters (K).\n\nCluster analysis & visualization\n\nCluster artifacts and model saved in code/ml/models/kmeans_k13.\nCluster analysis summary: data/csv/ML2_cluster_analysis.csv.\nElbow chart for K selection: data/plots/ML2_elbow_method.png.\n\n\nclustering: - The clustering plot groups subreddits by similar text/sentiment patterns. Key clusters include cluster 10 (largest, many tokens like ‘removed’, ‘ai’, ‘chatgpt’) and cluster 8 (conversational language). - These clusters can be used to build cluster-specific forecasting models or to include cluster id as a feature in regression/classification tasks.\nSelected cluster summary: - Largest cluster id 10: size 43,558 (tokens: ‘removed’, ‘im’, ‘ai’, ‘chatgpt’, ‘gpt’, ‘new’, ‘use’, ‘get’) - Cluster 8: size 4,989 (conversational tokens: ‘im’, ‘like’, ‘time’) - Cluster 6: size 511 (technical tokens: ‘ai’, ‘code’, ‘import’, ‘data’)\nInterpretation & current status: - The K-Means clustering successfully segmented the subreddits into meaningful groups based on their language. - Cluster 10 (Size: 43,558): The largest cluster, characterized by general discussion terms ('im', 'like'), major topics ('ai', 'chatgpt'), and moderation-related tokens ('removed'). This represents the bulk of high-volume, general-purpose conversations. - Cluster 8 (Size: 4,989): A significant cluster focused on conversational language ('ive', 'know', 'dont'), indicating a community of informal interaction. - Cluster 6 (Size: 511): A smaller but highly coherent technical cluster with terms like 'code', 'import', and 'data', clearly identifying programming-focused discussions. - These discovered clusters can serve as valuable features for more advanced models, such as building separate sentiment forecasting models for technical vs. conversational communities. - No regression model for next-month sentiment forecasting is saved in the repository; implementing this is recommended as the next step.\nNext steps to implement forecasting: 1. Aggregate monthly features per subreddit/cluster: lagged sentiment, topic shares, activity metrics. 2. Train and cross-validate regression models (GBRT, ElasticNet), evaluate with RMSE/MAE/R2. 3. Use interpretability tools (SHAP) to identify key drivers. 4. Deploy forecasting pipeline and produce monthly forecasts with uncertainty bounds."
  },
  {
    "objectID": "NLP_WEB.html",
    "href": "NLP_WEB.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "This study employs Natural Language Processing (NLP) techniques to investigate discussions related to Technology, Science, and Artificial Intelligence on Reddit. We analyze community discourse across multiple subreddits to identify dominant topics, track sentiment trends over time, and explore thematic differences in ethical, technical, and societal discussions. Methods include Latent Dirichlet Allocation (LDA) for topic modeling, VADER for sentiment analysis, and rule-based text classification to generate word clouds, providing both quantitative and visual insights into the semantic structure and community focus within AI- and technology-related discussions."
  },
  {
    "objectID": "NLP_WEB.html#introduction",
    "href": "NLP_WEB.html#introduction",
    "title": "Natural Language Processing",
    "section": "",
    "text": "This study employs Natural Language Processing (NLP) techniques to investigate discussions related to Technology, Science, and Artificial Intelligence on Reddit. We analyze community discourse across multiple subreddits to identify dominant topics, track sentiment trends over time, and explore thematic differences in ethical, technical, and societal discussions. Methods include Latent Dirichlet Allocation (LDA) for topic modeling, VADER for sentiment analysis, and rule-based text classification to generate word clouds, providing both quantitative and visual insights into the semantic structure and community focus within AI- and technology-related discussions."
  },
  {
    "objectID": "NLP_WEB.html#business-question-5-what-are-the-dominant-topics-and-trends-within-fast-growing-technology-related-subreddits",
    "href": "NLP_WEB.html#business-question-5-what-are-the-dominant-topics-and-trends-within-fast-growing-technology-related-subreddits",
    "title": "Natural Language Processing",
    "section": "Business Question 5: What are the dominant topics and trends within fast-growing technology-related subreddits?",
    "text": "Business Question 5: What are the dominant topics and trends within fast-growing technology-related subreddits?\nAs shown in the figure, the Latent Dirichlet Allocation (LDA) model identifies ten distinct topics from the corpus, each represented by its most influential keywords. These keywords provide an interpretable summary of the semantic focus of each discovered topic.\nTopic 0 is characterized by keywords such as data, apple, ai, science, user, and marketing, suggesting that this topic centers on discussions related to data science, artificial intelligence, and technology-driven business practices.\nTopic 1 includes terms such as karma, comment, subreddit, message, and questions, indicating that it reflects conversations about community interactions, platform moderation, and posting behaviors within online forums such as Reddit.\nTopic 2 is defined by words like human, ai, power, energy, car, and years, representing discourse around human–AI relations, emerging technologies, and long-term trends in energy and transportation.\nTopic 3 contains keywords including word, images, water, read, and data, pointing to content associated with information extraction, text and image processing, and data-driven analysis.\nTopic 4 focuses on terms such as code, python, data, using, language, and windows, clearly corresponding to programming-related discussions involving software tools, coding practices, and computational workflows.\nTopic 5 includes reddit, app, api, change, party, commercial, and pricing, which collectively suggest themes related to application development, API usage, and commercial product or platform changes.\nTopic 6 is composed of more conversational keywords such as something, never, someone, bad, lol, and yeah, indicating informal, colloquial exchanges or personal expressions within the corpus.\nTopic 7 features terms like learn, help, project, learning, software, and job, pointing to discussions focused on learning resources, technical support, project development, and career-oriented themes.\nTopic 8 is defined by job, jobs, entrepreneur, pay, business, and company, representing topics related to employment, entrepreneurship, compensation, and broader business activities.\nTopic 9 includes keywords such as chatgpt, bot, prompt, conversation, post, and reply, indicating discourse involving conversational agents, prompt design, and user–system interactions.\nOverall, these topics illustrate the LDA model’s ability to uncover coherent thematic structures within the dataset and to organize large volumes of text into interpretable clusters.\n\n\n\nTopic keywords\n\n\n\nOverall Topic Distribution\nAs shown in the pie chart, the Latent Dirichlet Allocation (LDA) model identifies ten distinct topics. Before ranking them, it is crucial to address Topic 6.\nA Note on Topic 6 (General Conversational Topic): Topic 6 (22.4%), with keywords like something, never, someone, bad, lol, yeah, represents a “background” or “general conversational” topic. It captures common, low-specificity words and conversational filler that are prevalent across the entire dataset but do not form a coherent semantic theme. The emergence of such a topic is a common and expected artifact of LDA. For the purpose of identifying the most significant thematic discussions, we will exclude Topic 6 from the subsequent ranking.\nAfter setting aside the general conversational topic, the most dominant thematic topics are as follows:\n\n\n\nOverall Topic Distribution\n\n\n\n1. Dominant Thematic Topics\n\nTopic 2 (13.6%)\nKeywords: human, ai, power, energy, car, years\nThis is the most prominent thematic topic, focusing on human–AI relations, emerging technologies, and long-term trends in energy and transportation. It highlights discussions centered on technological development and future-oriented themes.\nTopic 7 (13.0%)\nKeywords: learn, help, project, learning, software, job\nThis topic captures discourse around learning resources, technical support, project development, and career-related topics. It indicates active participation in educational and professional growth discussions within AI-focused communities.\nTopic 0 (11.8%)\nKeywords: data, apple, ai, science, user, marketing\nThis topic encompasses discussions at the intersection of technology and business, including data science, AI applications, and technology-driven business practices.\n\n\n\n2. Secondary Thematic Topics\n\nTopic 8 (11.3%)\nKeywords: job, jobs, entrepreneur, pay, business, company\nThis topic pertains to employment and entrepreneurship, reflecting user interest in career opportunities, compensation, and business activities.\nTopic 4 (11.0%)\nKeywords: code, python, data, using, language, windows\nThis topic represents programming and technical practice, emphasizing coding, software tools, and computational workflows.\n\n\n\n3. Peripheral Topics\n\nTopic 3 (7.7%)\nKeywords: word, images, water, read, data\nThis topic relates to information extraction, text and image processing, and data analysis.\nTopics 1, 5, 9 (&lt;5% each)\nThese topics include discussions about application development and APIs, community interactions, and conversational agents such as ChatGPT.\n\n\n\n4. Overall Interpretation\n\nAfter filtering out the large general conversational topic (Topic 6), the discourse is led by discussions on future-oriented technology and human-AI relations (Topic 2).\nCareer and learning-focused topics (Topics 7 and 8) remain highly significant, accounting for roughly 24% of the corpus and highlighting a strong user focus on professional development.\nTechnology and business-related discussions (Topics 0, 4) are also central, collectively representing over 22% of the content.\nThis revised distribution reveals that beneath a layer of casual conversation, the community’s core focus is on the future implications of technology, career growth, and practical applications in business and programming."
  },
  {
    "objectID": "NLP_WEB.html#business-question-6-what-are-the-baseline-emotional-patterns-of-discussions-about-ai-and-emerging-technologies",
    "href": "NLP_WEB.html#business-question-6-what-are-the-baseline-emotional-patterns-of-discussions-about-ai-and-emerging-technologies",
    "title": "Natural Language Processing",
    "section": "Business Question 6: What are the baseline emotional patterns of discussions about AI and emerging technologies?",
    "text": "Business Question 6: What are the baseline emotional patterns of discussions about AI and emerging technologies?\nMethod: We applied the VADER sentiment analysis tool to each comment and submission to calculate a compound score, which ranges from -1 (most negative) to +1 (most positive). Scores are then categorized as positive (&gt;=0.05), neutral, or negative (&lt;=-0.05).\nAnalysis of Sentiment Distribution: The stacked bar chart below shows the proportion of comments falling into each sentiment category for the most active subreddits.\n\n\n\nSentiment distribution\n\n\n\nFinding 1: Neutrality is Dominant. Across almost all communities, the vast majority of comments are classified as neutral. This indicates that discussions on technical and scientific topics are often objective, informative, and factual rather than emotionally charged.\nFinding 2: Positive Skew. In most subreddits, the proportion of positive comments is noticeably higher than negative ones. This suggests a generally constructive or optimistic underlying tone, even within objective discussions.\n\nAnalysis of Average Compound Score: The bar chart below visualizes the average compound sentiment score for each subreddit.\n\n\n\ncompound\n\n\n\nFinding 3: Mildly Positive Atmosphere. Consistent with the distribution analysis, the average sentiment score for most subreddits is slightly above zero, confirming a modest but persistent positive inclination. For example, communities like AIforGood and OpenAI show a stronger positive leaning.\nFinding 4: Lack of Strong Negativity. Very few communities exhibit a negative average sentiment, and even those that do are only slightly negative. This reinforces the conclusion that the overall discussion atmosphere is not contentious but rather balanced and leaning towards positive."
  },
  {
    "objectID": "NLP_WEB.html#business-question-7-how-do-external-technological-or-policy-events-disrupt-or-reshape-existing-discussion-patterns-in-online-technology-related-communities",
    "href": "NLP_WEB.html#business-question-7-how-do-external-technological-or-policy-events-disrupt-or-reshape-existing-discussion-patterns-in-online-technology-related-communities",
    "title": "Natural Language Processing",
    "section": "Business Question 7: How do external technological or policy events disrupt or reshape existing discussion patterns in online technology-related communities?",
    "text": "Business Question 7: How do external technological or policy events disrupt or reshape existing discussion patterns in online technology-related communities?\nWe selected a set of representative subreddits and categorized them into four major groups:\nAI/ML: This category includes ChatGPT, OpenAI, ArtificialIntelligence, MachineLearning, GenerativeAI, and AIethics. It focuses on discussions related to artificial intelligence, machine learning, generative AI, and associated ethical considerations.\nProgramming/Data: This group comprises datascience, bigdata, programming, Python, learnprogramming, and CloudComputing, highlighting topics in programming, data science, and technical learning.\nScience/STEM: Covering science, Physics, Engineering, Astronomy, Neuroscience, and MaterialsScience, this category addresses general science and STEM-related topics.\nTech/Future Trends: Including technology, Futurology, TechCulture, Innovation, and FutureTechnology, this category captures discussions about technological trends, innovation, and future-oriented topics.\nEach comment from these subreddits was analyzed using NLTK’s VADER to compute monthly average sentiment trends.\nIn the visualization, the vertical gray dashed lines represent key AI and technology events:\nClaude 2 launch (2023-07-12)\nOpenAI Developer Day (2023-11-06)\nGemini launch (2024-02-15)\nThe EU AI Act (2024-04-17)\nGPT-5 rumors (2024-06-20).\nThese lines serve as reference points to observe sentiment fluctuations around the time of significant events.\n\n\n\nSentiment trend\n\n\nAnalysis of Sentiment Trend: The chart below plots the average monthly sentiment for four distinct categories of subreddits. Key industry and policy events are marked with vertical lines to contextualize the trends.\n\n\n\nSentiment trend\n\n\n\nOverall Stability: The primary finding is that sentiment across all four categories remains remarkably stable over the year, with average scores consistently hovering in the neutral-to-mildly-positive range (0.05 to 0.15). This suggests a mature and steady discussion environment that is not prone to dramatic, long-term shifts in mood.\nEvent-Driven Fluctuations in AI/ML: The AI/ML category (blue line) displays the most sensitivity to external events.\n\nFollowing the Gemini launch (Feb 2024) and the EU AI Act (Apr 2024), this category shows a distinct upward trend, indicating a period of increased optimism and positive discussion.\nThis suggests that major product releases and significant regulatory milestones can temporarily boost positive sentiment within core AI communities.\n\nConsistency in Other Categories: The Programming/Data, Science/STEM, and Tech/Future Trends categories show even less volatility, reinforcing the idea that their discussion tones are less influenced by specific AI-related news cycles.\n\n\nTopic Trend\nWe also analyzed the distribution of the ten topics (discovered in RQ5) over time.\n\n\n\nTopic Trend\n\n\n\nConclusion: As shown in the stacked area chart, the relative prevalence of each topic remains highly consistent throughout the year. There are no major shifts, indicating that the fundamental areas of discussion within these communities are stable and not subject to seasonal or event-driven changes. For example, “informal conversation” (Topic 6) and “human-AI relations” (Topic 2) consistently remain the most dominant topics."
  },
  {
    "objectID": "NLP_WEB.html#business-question-8-how-do-users-shape-topic-emphasis-and-sentiment-dynamics-across-science-technology-and-ai-subreddits",
    "href": "NLP_WEB.html#business-question-8-how-do-users-shape-topic-emphasis-and-sentiment-dynamics-across-science-technology-and-ai-subreddits",
    "title": "Natural Language Processing",
    "section": "Business Question 8: How do users shape topic emphasis and sentiment dynamics across science, technology, and AI subreddits?",
    "text": "Business Question 8: How do users shape topic emphasis and sentiment dynamics across science, technology, and AI subreddits?\nTo explore the semantic characteristics of discussions related to Technology, Science, and AI, word clouds were created to visualize the most frequent and prominent terms within each topic group.\nDiscussion content was classified into four categories using a rule-based approach:\n\nTechnical: Text containing keywords related to model architectures, neural networks, training, or optimization.\nEthical: Text containing keywords related to ethics, bias, regulations, or fairness.\nSocietal: Text containing keywords related to societal impact, education, policy, or employment.\nOther: Text not matching any of the above patterns.\n\nThis approach allows the word clouds to intuitively highlight the main topics and language features within each category, providing a visual understanding of community focus.\n\n\n\nWordCloud\n\n\nAnalysis: The word clouds reveal clear thematic distinctions between the different categories of discussion:\n\nEthical Discussion: Prominent keywords include regulation, responsibility, human, bias, fairness, and moral. This indicates that discourse in this category centers on the governance, accountability, and human-centric implications of AI and technology.\nSocietal Discussion: Dominant words include job, work, company, society, future, and impact. This highlights a focus on the real-world consequences of technology, particularly concerning employment, corporate influence, and long-term social structures.\nTechnical Discussion: Key terms are model, data, training, LLM, neural, and architecture. This demonstrates a clear focus on the mechanics of AI development, including model design, data handling, and the specifics of training large language models.\nOther Discussion: This category is characterized by general or meta-discussion words like removed, know, need, work, and time. It likely captures content related to moderation, general inquiries, or conversations that do not fit neatly into the other three themes."
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Final Report",
    "section": "",
    "text": "This project investigates how online science, technology, and AI communities on Reddit evolve, interact, and structure their discussions around emerging technologies. Motivated by the overarching question of how these communities respond to rapid advances in AI and technology, we analyze more than 13.4 million comments and submissions from 40 curated subreddits between June 2023 and July 2024. Our goal is to uncover the temporal dynamics of community activity, patterns of user engagement, thematic structures of discussion, and the feasibility of predicting content quality through machine learning.\nTo support computation at this scale, we developed a distributed data-processing pipeline on an AWS EC2 Spark cluster, enabling efficient handling of over 13GB of text and metadata. Using this infrastructure, we apply an integrated analytical framework that combines Exploratory Data Analysis (EDA), Natural Language Processing (NLP), and Machine Learning (ML) to address ten core business questions. These analyses examine activity fluctuations, user retention cycles, sentiment stability, topic evolution, cross-community overlap, and the predictive modeling of high-quality content. Together, these components provide a multi-layered understanding of how Reddit’s technology and AI ecosystems function, evolve, and respond to external technological or policy events."
  },
  {
    "objectID": "report.html#project-summary",
    "href": "report.html#project-summary",
    "title": "Final Report",
    "section": "",
    "text": "This project investigates how online science, technology, and AI communities on Reddit evolve, interact, and structure their discussions around emerging technologies. Motivated by the overarching question of how these communities respond to rapid advances in AI and technology, we analyze more than 13.4 million comments and submissions from 40 curated subreddits between June 2023 and July 2024. Our goal is to uncover the temporal dynamics of community activity, patterns of user engagement, thematic structures of discussion, and the feasibility of predicting content quality through machine learning.\nTo support computation at this scale, we developed a distributed data-processing pipeline on an AWS EC2 Spark cluster, enabling efficient handling of over 13GB of text and metadata. Using this infrastructure, we apply an integrated analytical framework that combines Exploratory Data Analysis (EDA), Natural Language Processing (NLP), and Machine Learning (ML) to address ten core business questions. These analyses examine activity fluctuations, user retention cycles, sentiment stability, topic evolution, cross-community overlap, and the predictive modeling of high-quality content. Together, these components provide a multi-layered understanding of how Reddit’s technology and AI ecosystems function, evolve, and respond to external technological or policy events."
  },
  {
    "objectID": "report.html#detailed-findings",
    "href": "report.html#detailed-findings",
    "title": "Final Report",
    "section": "Detailed Findings",
    "text": "Detailed Findings\nThis section provides an integrated overview of insights from the three analytical components of the project: Exploratory Data Analysis (EDA), Natural Language Processing (NLP), and Machine Learning (ML). Each subsection summarizes the key discoveries and links to a dedicated analysis page that documents interactive visuals, methodological details, and complete result sets.\n\nExploratory Data Analysis (EDA)\nThe EDA characterizes structural patterns in community behavior across 40 AI and technology subreddits by examining activity dynamics, user retention, attention allocation, and cross-community participation. Across the 14 months, community activity displays a hybrid pattern: broad technology subreddits maintain high baseline volume, while AI-focused forums exhibit episodic spikes aligned with major releases and public announcements. This separation reflects different drivers of participation, general technological interest versus AI-specific event sensitivity.\nUser retention analysis further distinguishes communities by functional role. Mainstream AI subreddits (e.g., ChatGPT, MachineLearning) maintain stable return rates, indicating sustained month-to-month engagement. At the same time, niche or highly specialized communities show short, high-intensity retention bursts tied to topic relevance. These patterns indicate that community size and topic breadth shape long-term participation capacity.\nAttention distribution also varies systematically across subreddit types. High-traffic communities exhibit an intense concentration of engagement with comment activity disproportionately captured by a small number of threads consistent with viral or news-driven dynamics. In contrast, technical and research-oriented subreddits (such as Machine Learning and Data Science) show lower attention inequality, supporting more evenly distributed, parallel discussions that align with expert-driven or problem-solving interactions.\nFinally, user-overlap analysis reveals minimal cross-participation between AI and technology communities, even among subreddits addressing closely related topics. This indicates that Reddit’s AI ecosystem consists of distinct audience segments, each interacting primarily within its own topical domain rather than migrating across adjacent communities.\nTogether, these EDA results identify apparent structural differences in how AI and technology communities behave, including activity drivers, engagement stability, attention allocation, and user segmentation, providing the analytical foundation for the NLP and ML components of the project.\nSee full analysis in Exploratory Data Analysis\n\n\nNatural Language Processing (NLP)\nThe NLP analysis examines the semantic structure, emotional tone, event responsiveness, and user-driven discussion patterns across 40 AI and technology-related subreddits. Topic modeling reveals a stable set of thematic structures centered on future-oriented technology, human–AI relations, practical learning and project development, programming workflows, and technology-driven business applications. After excluding a significant general conversational topic, the remaining themes remain remarkably consistent throughout the 14 months, indicating that community interests are persistent and not subject to substantial drift. These findings suggest that discussions in fast-growing AI and technology communities are shaped by enduring concerns about technological impact, career development, and practical implementation rather than short-lived trends.\nSentiment analysis using VADER further demonstrates that the emotional tone across all major communities is predominantly neutral to mildly positive. Average sentiment scores remain stable over time, with only modest fluctuations in AI-focused subreddits around major product announcements or regulatory milestones such as the Gemini release or the EU AI Act. These shifts, however, are short-lived and do not meaningfully alter the long-term emotional landscape. Programming, science, and broader technology communities show even less sensitivity to external events, reinforcing the conclusion that discourse in these communities is informational and measured rather than reactive or volatile.\nTo understand how users contribute to different forms of discourse, discussions were categorized into technical, ethical, societal, and general content. Technical discussions emphasize model architectures, data workflows, and computational methods; ethical discussions focus on fairness, regulation, and governance; societal discussions address employment, corporate influence, and broader social implications; and general discussions reflect meta-talk, moderation, or uncategorized exchanges. These distinctions reveal parallel layers of conversation, each shaped by distinct user groups with different informational needs and priorities.\nOverall, the NLP analysis shows that Reddit’s AI and technology discussions form a stable, multidimensional discourse environment. Community conversations are thematically coherent, emotionally steady, and only marginally influenced by external technological or policy events. Users contribute across diverse topical layers—from technical implementation to societal impact—indicating a mature and sustained public engagement with emerging technologies.\nSee full analysis in Natural Language Processing\n\n\nMachine Learning (ML)\nThe machine learning analysis evaluates whether high-quality comments can be predicted from textual and behavioral features, and whether subreddits can be meaningfully segmented based on language use. For the predictive task, we formulate comment quality classification as a binary problem using a threshold of score &gt;= 6. A logistic regression model trained on TF-IDF features and lightweight metadata demonstrates modest discriminative performance. While the model achieves a high recall of over 70%, enabling it to identify the majority of high-quality comments, its precision is low, leading to a substantial number of false positives. The overall accuracy (0.50) and AUC (0.63) indicate that the classifier performs better than random but lacks the reliability required for automated decision-making. These results suggest that such a model is more appropriate as a triage or content-surfacing tool to assist human moderators rather than as a standalone quality-filtering system. They also highlight the limitations of sparse text representations and basic behavioral features for capturing the nuances of high-engagement content.\nTo complement the predictive task, we apply unsupervised learning to uncover structural patterns in language use across subreddits. K-Means clustering on TF-IDF embeddings reveals several distinct groups, including a large cluster dominated by broad conversational language and AI-related terms, a smaller cluster focused on informal dialogue, and a well-defined technical cluster characterized by programming-related terminology. These clusters demonstrate that subreddit content naturally organizes into coherent thematic groupings, validating the use of clustering to identify community types and communication styles. Such segmentation can support downstream applications, including building community-specific forecasting models, tailoring moderation strategies, or incorporating cluster identity as a feature in supervised models.\nTogether, the ML results show that while simple classification models offer limited predictive power, combining predictive and clustering approaches provides meaningful insights into content structure and community segmentation. Future improvements—such as replacing TF-IDF with contextual embeddings, incorporating richer author and subreddit history features, or using more advanced classifiers like gradient boosting are likely to yield significantly stronger performance. These advances would enable the transition from exploratory modeling toward more robust systems capable of supporting real-world moderation, recommendation, and content-quality assessment workflows.\nSee full analysis in Machine Learning"
  },
  {
    "objectID": "report.html#business-recommendations",
    "href": "report.html#business-recommendations",
    "title": "Final Report",
    "section": "Business Recommendations",
    "text": "Business Recommendations\nThe findings from this project highlight several opportunities for organizations seeking to understand better and engage with online science and technology communities. First, the strong event-driven dynamics observed across AI-related subreddits suggest that communication strategies should be aligned with major product releases, policy announcements, and industry milestones. Timely engagement during these high-visibility periods can amplify reach and influence. Second, the clear separation between niche technical communities and broader mainstream forums indicates that outreach and messaging should be tailored to each audience’s specific engagement structures. Technical communities benefit from sustained, information-rich content, while general-interest groups respond more strongly to concise, high-level narratives and trend-oriented discussions. Third, the high concentration of attention within viral posts in large subreddits underscores the importance of identifying and strategically participating in high-impact threads, as these disproportionately shape community sentiment and visibility.\nIn addition, the NLP results show that sentiment across communities remains stable and neutral, primarily positive, suggesting that discussions surrounding AI and emerging technologies create a constructive environment for public-facing communication. Organizations can leverage this stability by contributing expert perspectives, clarifications, or educational content without significant risk of entering volatile or hostile discourse. Finally, although machine learning classifiers show limited precision, their strong recall indicates potential value as triage tools within moderation or engagement workflows. Integrating predictive models to surface potentially high-value content, while maintaining human oversight, could improve efficiency in content curation and community management."
  },
  {
    "objectID": "report.html#future-work",
    "href": "report.html#future-work",
    "title": "Final Report",
    "section": "Future Work",
    "text": "Future Work\nExtensions of this project may enhance both analytical depth and operational applicability. From a modeling perspective, replacing TF-IDF features with contextual language embeddings, incorporating richer author-level and subreddit-level metadata, and experimenting with more advanced classifiers such as gradient boosting or transformer-based architectures would likely improve predictive performance. In unsupervised analysis, clustering could be expanded using hierarchical or density-based approaches to capture more nuanced community structures, and cluster identities could be integrated into sentiment or engagement forecasting models.\nAdditional longitudinal analyses may also provide insight into how community behavior evolves over multi-year periods, especially as AI technologies continue to advance. Integrating external datasets such as real-world AI release timelines, policy changes, or news sentiment could help establish causal links between external events and online discourse. Finally, developing a full forecasting pipeline for subreddit-level sentiment or activity trends would enable stakeholders to anticipate shifts in public attention, supporting more proactive communication strategies. Collectively, these extensions would deepen the analytical value of the project and expand its potential for operational use in community engagement, policy analysis, and emerging-technology monitoring."
  },
  {
    "objectID": "EDA_WEB.html",
    "href": "EDA_WEB.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This study examines activity dynamics, user engagement, attention concentration, and cross-community interactions across major AI and technology subreddits from June 2023 to July 2024. Through large-scale data processing with Spark and Python, we explore how community participation evolves over time, which subreddits retain returning users most effectively, how discussion attention is distributed across posts, and whether AI/tech communities share overlapping user bases. These analyses provide a foundational understanding of behavioral patterns within the broader Reddit AI ecosystem."
  },
  {
    "objectID": "EDA_WEB.html#data-overview",
    "href": "EDA_WEB.html#data-overview",
    "title": "Exploratory Data Analysis",
    "section": "Data Overview",
    "text": "Data Overview\n\nDataset Summary\nThis project analyzes a large-scale Reddit dataset covering user activity from June 2023 to July 2024 across 46 unique subreddits. The dataset captures both submissions and comments, enabling a comprehensive examination of engagement patterns, topic trends, and community dynamics over time. The two charts below illustrate the relevant content.\n\nTime range: 2023-06 – 2024-07\nTotal Comments:\nThe dataset contains a total of 12,561,291 comments collected from Reddit between June 2023 and July 2024. This large volume of user-generated content enables detailed temporal, thematic, and engagement-focused analyses.\nTotal Submissions:\nIn addition to comments, the dataset includes 831,021 submissions, representing the initial posts that anchor each discussion thread. These submissions provide essential context for understanding topic initiation patterns and community-level activity.\nTotal Subreddits:\nThe dataset spans 46 distinct subreddits, covering a diverse range of themes including technology, artificial intelligence, science, careers, and future-oriented discussions. This breadth allows for comparative analysis across communities with different interests and engagement behaviors.\nTop Subreddits by Total Activity\nActivity is measured by total_rows = num_comments + num_submissions. The most active subreddits in the dataset are:\n\ntechnology — 3,265,248 total rows\n\nChatGPT — 1,894,755 total rows\n\nscience — 1,051,336 total rows\n\ncscareerquestions — 974,990 total rows\n\nFuturology — 914,347 total rows\n\nThese subreddits account for a significant share of the dataset’s total activity, reflecting strong user engagement in discussions related to emerging technologies, AI tools, scientific topics, and career development.\n\n\n\n\n\n\n\nDataset Summary Table\n\n\n\ndata_type\ntotal_rows\nsize_gb\ndate_range_start\ndate_range_end\n\n\n\n\n0\ncomments\n12561291\n12.56\n2023-06-01\n2024-07-31\n\n\n1\nsubmissions\n831021\n0.83\n2023-06-01\n2024-07-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubreddit\nnum_comments\navg_comment_score\nnum_submissions\navg_submission_score\ntotal_rows\n\n\n\n\n44\ntechnology\n3210257\n11.109351\n54991\n337.551036\n3265248\n\n\n4\nChatGPT\n1748893\n6.564207\n145862\n51.257984\n1894755\n\n\n40\nscience\n1028744\n9.878945\n22592\n354.457596\n1051336\n\n\n28\ncscareerquestions\n910865\n5.916743\n64125\n12.182316\n974990\n\n\n12\nFuturology\n898463\n6.395128\n15884\n176.512465\n914347\n\n\n9\nEntrepreneur\n755790\n2.477290\n86534\n4.859373\n842324\n\n\n45\nwebdev\n516710\n4.481493\n57369\n8.473967\n574079\n\n\n35\nlearnprogramming\n452260\n3.041514\n50429\n5.591862\n502689\n\n\n19\nOpenAI\n337768\n4.450842\n25574\n24.268476\n363342\n\n\n38\nprogramming\n325140\n8.167888\n31396\n20.063256\n356536\n\n\n32\ngadgets\n331981\n8.062708\n6434\n254.383898\n338415\n\n\n36\nlearnpython\n208024\n2.108420\n30948\n2.720628\n238972\n\n\n8\nElectricalEngineering\n214790\n4.132976\n22407\n8.276208\n237197\n\n\n43\ntechnews\n222603\n5.017843\n9896\n147.518795\n232499\n\n\n2\nArtificialInteligence\n193665\n2.013590\n27614\n3.891903\n221279\n\n\n29\ndatascience\n193664\n4.106711\n24717\n6.360278\n218381\n\n\n16\nMachineLearning\n146901\n3.897986\n37191\n4.729182\n184092\n\n\n20\nPhysics\n166907\n5.138023\n16552\n11.204265\n183459\n\n\n18\nMechanicalEngineering\n142210\n4.528043\n10692\n8.584175\n152902\n\n\n3\nAstronomy\n114814\n5.744718\n13305\n41.608794\n128119\n\n\n21\nPython\n100149\n3.893229\n18323\n6.637396\n118472\n\n\n42\ntech\n78069\n4.071629\n6321\n109.124347\n84390\n\n\n37\nphysicsmemes\n62492\n13.372304\n3764\n242.282147\n66256\n\n\n39\nrobotics\n44127\n2.407347\n7575\n9.391551\n51702\n\n\n27\ncomputerscience\n31181\n3.723197\n7504\n3.898188\n38685\n\n\n30\ndeeplearning\n29739\n1.890346\n5534\n3.626671\n35273\n\n\n34\nlearnjava\n23050\n1.671584\n3392\n2.901238\n26442\n\n\n5\nComputerEngineering\n14360\n2.661490\n2563\n4.098712\n16923\n\n\n23\nQuantumPhysics\n11850\n1.812911\n1668\n3.244005\n13518\n\n\n22\nQuantumComputing\n11155\n1.864186\n1786\n4.209966\n12941\n\n\n15\nMLQuestions\n7726\n1.470230\n3094\n1.648675\n10820\n\n\n41\nspaceflight\n8408\n2.907112\n1707\n12.414177\n10115\n\n\n26\ncoding\n5564\n2.204889\n4081\n2.046557\n9645\n\n\n11\nFuturism\n7000\n1.429000\n1042\n8.631478\n8042\n\n\n13\nInnovation\n392\n1.168367\n4995\n1.061061\n5387\n\n\n17\nMaterialsScience\n2075\n1.902169\n589\n2.558574\n2664\n\n\n25\nbigdata\n864\n1.261574\n1255\n1.521116\n2119\n\n\n0\nAI_Agents\n1270\n1.277165\n594\n2.028620\n1864\n\n\n7\nEarthScience\n829\n1.917973\n351\n2.934473\n1180\n\n\n14\nLLM\n349\n1.137536\n89\n0.561798\n438\n\n\n33\ngenai\n90\n1.044444\n182\n1.730769\n272\n\n\n6\nDataScienceMemes\n44\n1.068182\n86\n3.127907\n130\n\n\n1\nAIethics\n45\n1.044444\n7\n0.857143\n52\n\n\n10\nEnvironmentalScience\n11\n1.000000\n0\n0.000000\n11\n\n\n31\nfuturescience\n0\n0.000000\n7\n1.000000\n7\n\n\n24\nTechEntrepreneur\n3\n1.000000\n0\n0.000000\n3"
  },
  {
    "objectID": "EDA_WEB.html#rq1-how-has-community-activity-evolved-across-ai-and-technology-subreddits-over-time",
    "href": "EDA_WEB.html#rq1-how-has-community-activity-evolved-across-ai-and-technology-subreddits-over-time",
    "title": "Exploratory Data Analysis",
    "section": "RQ1: How has community activity evolved across AI and technology subreddits over time?",
    "text": "RQ1: How has community activity evolved across AI and technology subreddits over time?\nBusiness Question:\nHow have monthly post and comment volumes changed across major AI and technology subreddits? Do any noticeable surges correspond to major AI or technology events?\nAnalysis Approach:\n- Use Spark to aggregate posts and comments by subreddit and month\n- Visualize monthly activity trends to compare engagement patterns and identify periods of rapid growth or decline\n\n\n\n\n\n\n\n\n\nFindings:\n- technology consistently dominates overall activity, peaking above 300,000 monthly interactions in mid-2023 and again in mid-2024, reflecting its broad and sustained user interest.\n- ChatGPT shows several pronounced surges—particularly around late 2023 and early 2024—likely corresponding to major AI announcements and product updates.\n- Futurology exhibits a steady upward trend throughout the year, suggesting growing interest in future-oriented and AI-related discussions.\n- OpenAI experiences sharp but short-lived spikes (e.g., late 2023 and mid-2024), consistent with event-driven engagement tied to major releases or public statements.\n- Smaller communities such as MachineLearning, technews, and ArtificialInteligence remain relatively stable, showing modest fluctuations but no large-scale surges.\nOverall, activity patterns indicate that major AI/tech events tend to drive short-term spikes in engagement, while broader technology interest maintains high baseline levels throughout the entire period."
  },
  {
    "objectID": "EDA_WEB.html#rq2-which-ai-related-subreddits-demonstrate-the-strongest-user-engagement-and-retention-over-time",
    "href": "EDA_WEB.html#rq2-which-ai-related-subreddits-demonstrate-the-strongest-user-engagement-and-retention-over-time",
    "title": "Exploratory Data Analysis",
    "section": "RQ2: Which AI-related subreddits demonstrate the strongest user engagement and retention over time?",
    "text": "RQ2: Which AI-related subreddits demonstrate the strongest user engagement and retention over time?\nBusiness Question:\nWhich AI communities successfully retain users across months—indicating sustained engagement—rather than attracting only short-term or event-driven traffic?\nAnalysis Approach:\n- Use Spark to compute the monthly returning-to-active user ratio for each subreddit as a proxy for retention.\n- Visualize retention trends with a heatmap to highlight which communities maintain consistent returning user activity over time.\n\n\n\n\n\n\n\n\n\nFindings:\n\nNiche AI communities exhibit sharp fluctuations in retention.\nSubreddits with smaller and more specialized audiences—such as Alethics—show periods of extremely high retention, with ratios reaching up to ~1.0 in certain months. This indicates that when these communities experience relevant discussions or bursts of interest, the majority of users who are active in those months tend to return. However, these peaks are intermittent, suggesting that engagement in niche communities is event-driven and highly sensitive to topical relevance rather than sustained momentum.\nMainstream AI subreddits maintain moderate but stable retention levels.\nLarger communities like ChatGPT, MachineLearning, and OpenAI display retention ratios that consistently fall within the 0.2–0.4 range. Although these values are lower than those of niche subreddits during their peak months, their stability reflects a healthier and more durable engagement structure: a substantial portion of users continue returning month over month, supported by continual content flow, broader topic coverage, and steady user inflow.\nTemporal patterns reveal community-specific engagement cycles.\nRetention for niche subreddits tends to be more volatile, with visible month-to-month jumps. This volatility suggests communities formed around narrow or emerging topics experience engagement surges only when new developments occur. In contrast, mainstream subreddits exhibit smoother seasonal variations that correspond more closely to long-term trends in AI and technology discourse.\nGeneral AI forums demonstrate stronger baseline cohesion.\nCommunities like MachineLearning and OpenAI do not show extreme highs or lows but instead maintain a persistent core of recurring users. This indicates well-established community identity and ongoing discussions that encourage users to return even outside of major events.\nOverall interpretation:\nThe heatmap highlights a clear divide between niche and broad AI communities. Niche subreddits achieve high short-term engagement but lack consistency, while mainstream subreddits sustain steady returning-user activity, suggesting stronger community cohesion and long-term user retention. These patterns underline how topic breadth, community size, and event-driven interest all influence user retention dynamics across Reddit’s AI ecosystem."
  },
  {
    "objectID": "EDA_WEB.html#rq3-how-concentrated-is-attention-within-ai-and-tech-discussions",
    "href": "EDA_WEB.html#rq3-how-concentrated-is-attention-within-ai-and-tech-discussions",
    "title": "Exploratory Data Analysis",
    "section": "RQ3: How concentrated is attention within AI and tech discussions?",
    "text": "RQ3: How concentrated is attention within AI and tech discussions?\nBusiness Question:\nTo what extent are conversations driven by a small number of highly viral posts, versus being distributed more evenly across many posts? In other words, do certain subreddits exhibit stronger attention inequality than others?\nAnalysis Approach:\n- Use Spark to compute the Gini coefficient of comments per post for each subreddit, where higher Gini values indicate more concentrated attention.\n- Visualize distribution patterns using violin plots to show variability across months and bar charts to compare average attention concentration between subreddits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFindings:\n\nTechnology and ChatGPT exhibit the strongest concentration of attention, with average Gini values approaching 0.88–0.90.\nThis indicates that engagement in these subreddits is dominated by a handful of highly viral posts. When major announcements, controversies, or breakthrough AI tools emerge, these communities often experience rapid surges in attention focused on a small number of trending threads. As a result, the distribution of comments becomes heavily skewed: most posts receive modest engagement while a small set of “blockbuster” discussions absorb the majority of user activity. Such dynamics are typical of large, heterogeneous audiences that respond quickly to broad-interest or news-driven content.\nProfessional and technical communities—including MachineLearning and datascience—show significantly lower attention inequality (Gini ≈ 0.70–0.78).\nThese subreddits display a more evenly distributed engagement pattern, where comments accumulate across a wider range of posts rather than concentrating around a few viral threads. This suggests that discussions are more specialized, sustained, and topic-focused, attracting recurring contributions from users who are participating for informational or expert-driven purposes rather than reacting to viral events. The lower Gini values imply that these communities support richer, multi-thread parallel discussions typical of technical forums.\nOpenAI occupies a middle ground, with moderate concentration levels (~0.75–0.80) but higher variability across months.\nThis pattern reflects the community’s sensitivity to event-driven cycles: during major OpenAI releases or controversies, a few posts dominate the discourse, while in quieter periods, engagement becomes more evenly distributed.\nOverall, the Gini-based comparisons highlight a structural divide between general-interest and expert-focused AI communities.\nLarge, news-oriented subreddits tend to exhibit “winner-take-most” behavior, where viral content disproportionately shapes the conversation. In contrast, subreddits centered on technical learning, research, or professional practice support more equitable attention patterns, indicating a healthier distribution of dialogue across diverse posts. These findings reveal how audience composition, content type, and topic breadth jointly influence the way discussions unfold within online AI and technology communities."
  },
  {
    "objectID": "EDA_WEB.html#rq4-do-ai-and-technology-subreddits-share-overlapping-user-communities",
    "href": "EDA_WEB.html#rq4-do-ai-and-technology-subreddits-share-overlapping-user-communities",
    "title": "Exploratory Data Analysis",
    "section": "RQ4: Do AI and technology subreddits share overlapping user communities?",
    "text": "RQ4: Do AI and technology subreddits share overlapping user communities?\nBusiness Question:\nTo what extent do users participate across multiple AI and technology subreddits? Are some communities closely interconnected through shared user bases, while others attract distinct, non-overlapping audiences?\nAnalysis Approach:\n\nExtract unique users from each subreddit using Spark.\nCompute pairwise Jaccard similarity to quantify user overlap.\nVisualize the subreddit-to-subreddit similarity matrix using a heatmap to highlight shared and distinct audience patterns.\n\n\n\n\n\n\n\n\n\n\nFindings:\n\nOverall user overlap across the selected AI and technology subreddits is low, with most Jaccard similarity values close to zero. This indicates that these communities attract largely distinct audiences rather than sharing a common user base.\nGeneral-interest subreddits such as technology and technews show only weak connections to more specialized communities (e.g., MachineLearning, robotics, OpenAI), suggesting that mainstream tech discussions do not substantially overlap with expert-driven or AI-focused forums.\nSimilarly, AI-related subreddits such as ChatGPT, OpenAI, and ArtificialInteligence do not exhibit strong user overlap with one another, implying that users tend to engage in platform- or topic-specific communities rather than participating broadly across the AI ecosystem.\nThe near-diagonal heatmap pattern reinforces the idea that each subreddit represents a relatively independent audience segment, with minimal cross-community engagement or user migration during the study period."
  },
  {
    "objectID": "EDA_WEB.html#key-takeaways",
    "href": "EDA_WEB.html#key-takeaways",
    "title": "Exploratory Data Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCommunity activity is highly event-driven, with major AI/tech announcements producing clear surges in monthly posts and comments. Among all communities, the technology subreddit consistently maintains the highest baseline activity, highlighting its broad relevance and responsiveness to industry-wide events.\nUser retention patterns differ sharply across subreddit types. Specialized communities (e.g., science, robotics) show intermittent but intense periods of returning-user engagement, whereas larger mainstream AI communities such as ChatGPT and technology exhibit more stable, steady retention, indicating continuous community participation.\nAttention concentration varies with community purpose. General tech forums display high Gini coefficients (≈0.9), suggesting a “viral hit” structure where a few posts dominate engagement. In contrast, technical and research-focused subreddits (e.g., MachineLearning) distribute attention more evenly (Gini ≈0.7), supporting sustained, multi-threaded discussions rather than viral dynamics.\nCross-community user overlap is minimal, showing that AI and technology subreddits maintain largely distinct audience groups. Even closely related AI subreddits share limited user participation, indicating strong topical specialization rather than fluid cross-community engagement."
  }
]